{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwte6144sFeagH1qc9R9Vy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaran786/Wipro-s-Sustainability-Machine-Learning-Challenge/blob/main/TF2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WCBsZuTem8pP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers, datasets\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
        "\n",
        "def mnist_dataset():\n",
        "  (x, y), _ = datasets.mnist.load_data()\n",
        "  ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  ds = ds.map(prepare_mnist_features_and_labels)\n",
        "  ds = ds.take(20000).shuffle(20000).batch(100)\n",
        "  return ds\n",
        "\n",
        "def prepare_mnist_features_and_labels(x, y):\n",
        "  x = tf.cast(x, tf.float32) / 255.0\n",
        "  y = tf.cast(y, tf.int64)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(10)])\n",
        "\n",
        "optimizer = optimizers.Adam()"
      ],
      "metadata": {
        "id": "tuuHRT7YnS8W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_loss(logits, labels):\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=labels))\n",
        "\n",
        "@tf.function\n",
        "def compute_accuracy(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
        "\n",
        "@tf.function\n",
        "def train_one_step(model, optimizer, x, y):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x)\n",
        "    loss = compute_loss(logits, y)\n",
        "\n",
        "  # compute gradient\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  # update to weights\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  accuracy = compute_accuracy(logits, y)\n",
        "\n",
        "  # loss and accuracy is scalar tensor\n",
        "  return loss, accuracy\n",
        "\n",
        "def train(epoch, model, optimizer):\n",
        "\n",
        "  train_ds = mnist_dataset()\n",
        "  loss = 0.0\n",
        "  accuracy = 0.0\n",
        "  for step, (x, y) in enumerate(train_ds):\n",
        "    loss, accuracy = train_one_step(model, optimizer, x, y)\n",
        "\n",
        "    if step % 500 == 0:\n",
        "      print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
        "\n",
        "  return loss, accuracy"
      ],
      "metadata": {
        "id": "Sd1pqIZKnWzU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "  loss, accuracy = train(epoch, model, optimizer)\n",
        "\n",
        "print('Final epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZaXAsdHnYPm",
        "outputId": "1da914b6-36ff-4470-e453-9b604714c1c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "epoch 0 : loss 2.346877 ; accuracy 0.12\n",
            "epoch 1 : loss 0.1963037 ; accuracy 0.95\n",
            "epoch 2 : loss 0.17062595 ; accuracy 0.95\n",
            "epoch 3 : loss 0.053198777 ; accuracy 1.0\n",
            "epoch 4 : loss 0.051740132 ; accuracy 0.98\n",
            "epoch 5 : loss 0.035717674 ; accuracy 0.99\n",
            "epoch 6 : loss 0.062364575 ; accuracy 0.98\n",
            "epoch 7 : loss 0.023186618 ; accuracy 0.99\n",
            "epoch 8 : loss 0.04845501 ; accuracy 0.99\n",
            "epoch 9 : loss 0.012708366 ; accuracy 1.0\n",
            "epoch 10 : loss 0.011242194 ; accuracy 1.0\n",
            "epoch 11 : loss 0.008616026 ; accuracy 1.0\n",
            "epoch 12 : loss 0.012709178 ; accuracy 1.0\n",
            "epoch 13 : loss 0.0105299875 ; accuracy 1.0\n",
            "epoch 14 : loss 0.076283365 ; accuracy 0.99\n",
            "epoch 15 : loss 0.0027582068 ; accuracy 1.0\n",
            "epoch 16 : loss 0.0067415237 ; accuracy 1.0\n",
            "epoch 17 : loss 0.003963392 ; accuracy 1.0\n",
            "epoch 18 : loss 0.00092174864 ; accuracy 1.0\n",
            "epoch 19 : loss 0.0006610576 ; accuracy 1.0\n",
            "Final epoch 19 : loss 0.0015028993 ; accuracy 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con2"
      ],
      "metadata": {
        "id": "7ooaVTE5n5oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models, optimizers, metrics\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
        "\n",
        "\n",
        "def mnist_datasets():\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "    # Numpy defaults to dtype=float64; TF defaults to float32. Stick with float32.\n",
        "    x_train, x_test = x_train / np.float32(255), x_test / np.float32(255)\n",
        "    y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "train_ds, test_ds = mnist_datasets()\n",
        "train_ds = train_ds.shuffle(60000).batch(100)\n",
        "test_ds = test_ds.batch(100)"
      ],
      "metadata": {
        "id": "O2xYj5cjn5F9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Reshape(\n",
        "        target_shape=[28, 28, 1],\n",
        "        input_shape=(28, 28,)),\n",
        "    layers.Conv2D(2, 5, padding='same', activation=tf.nn.relu),\n",
        "    layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
        "    layers.Conv2D(4, 5, padding='same', activation=tf.nn.relu),\n",
        "    layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation=tf.nn.relu),\n",
        "    layers.Dropout(rate=0.4),\n",
        "    layers.Dense(10)])\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "0zNmDOISn_MG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "compute_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "def train_step(model, optimizer, images, labels):\n",
        "\n",
        "    # Record the operations used to compute the loss, so that the gradient\n",
        "    # of the loss with respect to the variables can be computed.\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(images, training=True)\n",
        "        loss = compute_loss(labels, logits)\n",
        "        compute_accuracy(labels, logits)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train(model, optimizer, dataset, log_freq=50):\n",
        "    \"\"\"\n",
        "    Trains model on `dataset` using `optimizer`.\n",
        "    \"\"\"\n",
        "    # Metrics are stateful. They accumulate values and return a cumulative\n",
        "    # result when you call .result(). Clear accumulated values with .reset_states()\n",
        "    avg_loss = metrics.Mean('loss', dtype=tf.float32)\n",
        "\n",
        "    # Datasets can be iterated over like any other Python iterable.\n",
        "    for images, labels in dataset:\n",
        "        loss = train_step(model, optimizer, images, labels)\n",
        "        avg_loss(loss)\n",
        "\n",
        "        if tf.equal(optimizer.iterations % log_freq, 0):\n",
        "            # summary_ops_v2.scalar('loss', avg_loss.result(), step=optimizer.iterations)\n",
        "            # summary_ops_v2.scalar('accuracy', compute_accuracy.result(), step=optimizer.iterations)\n",
        "            print('step:', int(optimizer.iterations),\n",
        "                  'loss:', avg_loss.result().numpy(),\n",
        "                  'acc:', compute_accuracy.result().numpy())\n",
        "            avg_loss.reset_states()\n",
        "            compute_accuracy.reset_states()\n",
        "\n",
        "def test(model, dataset, step_num):\n",
        "    \"\"\"\n",
        "    Perform an evaluation of `model` on the examples from `dataset`.\n",
        "    \"\"\"\n",
        "    avg_loss = metrics.Mean('loss', dtype=tf.float32)\n",
        "\n",
        "    for (images, labels) in dataset:\n",
        "        logits = model(images, training=False)\n",
        "        avg_loss(compute_loss(labels, logits))\n",
        "        compute_accuracy(labels, logits)\n",
        "\n",
        "    print('Model test set loss: {:0.4f} accuracy: {:0.2f}%'.format(\n",
        "        avg_loss.result(), compute_accuracy.result() * 100))\n",
        "\n",
        "    print('loss:', avg_loss.result(), 'acc:', compute_accuracy.result())\n",
        "    # summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
        "    # summary_ops_v2.scalar('accuracy', compute_accuracy.result(), step=step_num)"
      ],
      "metadata": {
        "id": "tEn7zvA2oDID"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where to save checkpoints, tensorboard summaries, etc.\n",
        "MODEL_DIR = '/tmp/tensorflow/mnist'\n",
        "\n",
        "\n",
        "def apply_clean():\n",
        "    if tf.io.gfile.exists(MODEL_DIR):\n",
        "        print('Removing existing model dir: {}'.format(MODEL_DIR))\n",
        "        tf.io.gfile.rmtree(MODEL_DIR)\n",
        "\n",
        "\n",
        "apply_clean()\n",
        "\n",
        "checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "\n",
        "# Restore variables on creation if a checkpoint exists.\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6hP98rjoZe1",
        "outputId": "a49c06e9-3320-4daa-b7ab-372cf765fcd1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f8958a68810>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRAIN_EPOCHS = 5\n",
        "\n",
        "for i in range(NUM_TRAIN_EPOCHS):\n",
        "    start = time.time()\n",
        "    #   with train_summary_writer.as_default():\n",
        "    train(model, optimizer, train_ds, log_freq=500)\n",
        "    end = time.time()\n",
        "    print('Train time for epoch #{} ({} total steps): {}'.format(\n",
        "        i + 1, int(optimizer.iterations), end - start))\n",
        "    #   with test_summary_writer.as_default():\n",
        "    #     test(model, test_ds, optimizer.iterations)\n",
        "    checkpoint.save(checkpoint_prefix)\n",
        "    print('saved checkpoint.')\n",
        "\n",
        "export_path = os.path.join(MODEL_DIR, 'export')\n",
        "tf.saved_model.save(model, export_path)\n",
        "print('saved SavedModel for exporting.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjHIZZ8toLN9",
        "outputId": "c67e4da6-ee47-4e2f-f627-c9e601f704c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1000 loss: 0.5799344 acc: 0.80838\n",
            "Train time for epoch #1 (1200 total steps): 12.985946416854858\n",
            "saved checkpoint.\n",
            "step: 1500 loss: 0.45264322 acc: 0.85226\n",
            "Train time for epoch #2 (1800 total steps): 13.080891609191895\n",
            "saved checkpoint.\n",
            "step: 2000 loss: 0.3895217 acc: 0.8755\n",
            "Train time for epoch #3 (2400 total steps): 12.943011999130249\n",
            "saved checkpoint.\n",
            "step: 2500 loss: 0.34031543 acc: 0.8901\n",
            "step: 3000 loss: 0.32634944 acc: 0.89968\n",
            "Train time for epoch #4 (3000 total steps): 13.0113205909729\n",
            "saved checkpoint.\n",
            "step: 3500 loss: 0.29977822 acc: 0.90822\n",
            "Train time for epoch #5 (3600 total steps): 14.014578342437744\n",
            "saved checkpoint.\n",
            "INFO:tensorflow:Assets written to: /tmp/tensorflow/mnist/export/assets\n",
            "saved SavedModel for exporting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression using boston dataset"
      ],
      "metadata": {
        "id": "KbLxyomJpLOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  tensorflow as tf\n",
        "import  numpy as np\n",
        "from    tensorflow import keras\n",
        "import  os\n",
        "\n",
        "\n",
        "class Regressor(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "\n",
        "        # here must specify shape instead of tensor !\n",
        "        # name here is meanless !\n",
        "        # [dim_in, dim_out]\n",
        "        self.w = self.add_variable('meanless-name', [13, 1])\n",
        "        # [dim_out]\n",
        "        self.b = self.add_variable('meanless-name', [1])\n",
        "\n",
        "        print(self.w.shape, self.b.shape)\n",
        "        print(type(self.w), tf.is_tensor(self.w), self.w.name)\n",
        "        print(type(self.b), tf.is_tensor(self.b), self.b.name)\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        x = tf.matmul(x, self.w) + self.b\n",
        "\n",
        "        return x\n",
        "\n",
        "def main():\n",
        "\n",
        "    tf.random.set_seed(22)\n",
        "    np.random.seed(22)\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    assert tf.__version__.startswith('2.')\n",
        "\n",
        "\n",
        "    (x_train, y_train), (x_val, y_val) = keras.datasets.boston_housing.load_data()\n",
        "    #\n",
        "    x_train, x_val = x_train.astype(np.float32), x_val.astype(np.float32)\n",
        "    # (404, 13) (404,) (102, 13) (102,)\n",
        "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
        "    # Here has two mis-leading issues:\n",
        "    # 1. (x_train, y_train) cant be written as [x_train, y_train]\n",
        "    # 2.\n",
        "    db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64)\n",
        "    db_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(102)\n",
        "\n",
        "\n",
        "    model = Regressor()\n",
        "    criteon = keras.losses.MeanSquaredError()\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "\n",
        "    for epoch in range(200):\n",
        "\n",
        "        for step, (x, y) in enumerate(db_train):\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # [b, 1]\n",
        "                logits = model(x)\n",
        "                # [b]\n",
        "                logits = tf.squeeze(logits, axis=1)\n",
        "                # [b] vs [b]\n",
        "                loss = criteon(y, logits)\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        print(epoch, 'loss:', loss.numpy())\n",
        "\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "\n",
        "            for x, y in db_val:\n",
        "                # [b, 1]\n",
        "                logits = model(x)\n",
        "                # [b]\n",
        "                logits = tf.squeeze(logits, axis=1)\n",
        "                # [b] vs [b]\n",
        "                loss = criteon(y, logits)\n",
        "\n",
        "                print(epoch, 'val loss:', loss.numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-a179LYpKpf",
        "outputId": "dbe13521-8210-40de-b42e-29fdbe97be65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n",
            "(404, 13) (404,) (102, 13) (102,)\n",
            "(13, 1) (1,)\n",
            "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'> True meanless-name:0\n",
            "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'> True meanless-name:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss: 39694.82\n",
            "0 val loss: 36500.332\n",
            "1 loss: 18709.125\n",
            "2 loss: 6534.4365\n",
            "3 loss: 1271.5076\n",
            "4 loss: 51.908943\n",
            "5 loss: 236.74028\n",
            "6 loss: 401.17242\n",
            "7 loss: 293.0791\n",
            "8 loss: 127.253494\n",
            "9 loss: 48.899605\n",
            "10 loss: 42.268044\n",
            "10 val loss: 124.677574\n",
            "11 loss: 49.233555\n",
            "12 loss: 47.356422\n",
            "13 loss: 42.244324\n",
            "14 loss: 39.967552\n",
            "15 loss: 39.822304\n",
            "16 loss: 39.75223\n",
            "17 loss: 39.398033\n",
            "18 loss: 39.11087\n",
            "19 loss: 38.935944\n",
            "20 loss: 38.73898\n",
            "20 val loss: 120.31869\n",
            "21 loss: 38.484207\n",
            "22 loss: 38.20948\n",
            "23 loss: 37.937237\n",
            "24 loss: 37.66734\n",
            "25 loss: 37.395897\n",
            "26 loss: 37.119164\n",
            "27 loss: 36.833473\n",
            "28 loss: 36.537872\n",
            "29 loss: 36.234398\n",
            "30 loss: 35.925823\n",
            "30 val loss: 114.442184\n",
            "31 loss: 35.613987\n",
            "32 loss: 35.299515\n",
            "33 loss: 34.98244\n",
            "34 loss: 34.66285\n",
            "35 loss: 34.341145\n",
            "36 loss: 34.01806\n",
            "37 loss: 33.694298\n",
            "38 loss: 33.370464\n",
            "39 loss: 33.047005\n",
            "40 loss: 32.724236\n",
            "40 val loss: 107.796234\n",
            "41 loss: 32.402504\n",
            "42 loss: 32.082157\n",
            "43 loss: 31.763535\n",
            "44 loss: 31.446945\n",
            "45 loss: 31.132675\n",
            "46 loss: 30.820963\n",
            "47 loss: 30.512033\n",
            "48 loss: 30.206066\n",
            "49 loss: 29.90329\n",
            "50 loss: 29.603832\n",
            "50 val loss: 101.22729\n",
            "51 loss: 29.307867\n",
            "52 loss: 29.015533\n",
            "53 loss: 28.726963\n",
            "54 loss: 28.442266\n",
            "55 loss: 28.161535\n",
            "56 loss: 27.884878\n",
            "57 loss: 27.612381\n",
            "58 loss: 27.344107\n",
            "59 loss: 27.08013\n",
            "60 loss: 26.82051\n",
            "60 val loss: 95.07062\n",
            "61 loss: 26.565287\n",
            "62 loss: 26.31451\n",
            "63 loss: 26.068232\n",
            "64 loss: 25.826466\n",
            "65 loss: 25.589252\n",
            "66 loss: 25.356592\n",
            "67 loss: 25.128517\n",
            "68 loss: 24.905037\n",
            "69 loss: 24.686153\n",
            "70 loss: 24.471867\n",
            "70 val loss: 89.49144\n",
            "71 loss: 24.262169\n",
            "72 loss: 24.057056\n",
            "73 loss: 23.856516\n",
            "74 loss: 23.660543\n",
            "75 loss: 23.469097\n",
            "76 loss: 23.282177\n",
            "77 loss: 23.099749\n",
            "78 loss: 22.92178\n",
            "79 loss: 22.748236\n",
            "80 loss: 22.579094\n",
            "80 val loss: 84.54923\n",
            "81 loss: 22.414312\n",
            "82 loss: 22.253853\n",
            "83 loss: 22.09767\n",
            "84 loss: 21.94572\n",
            "85 loss: 21.797953\n",
            "86 loss: 21.654339\n",
            "87 loss: 21.514812\n",
            "88 loss: 21.379324\n",
            "89 loss: 21.247824\n",
            "90 loss: 21.120268\n",
            "90 val loss: 80.235855\n",
            "91 loss: 20.996588\n",
            "92 loss: 20.876734\n",
            "93 loss: 20.76064\n",
            "94 loss: 20.648258\n",
            "95 loss: 20.539536\n",
            "96 loss: 20.434393\n",
            "97 loss: 20.33279\n",
            "98 loss: 20.23465\n",
            "99 loss: 20.139923\n",
            "100 loss: 20.048542\n",
            "100 val loss: 76.50178\n",
            "101 loss: 19.960451\n",
            "102 loss: 19.875582\n",
            "103 loss: 19.793869\n",
            "104 loss: 19.71526\n",
            "105 loss: 19.63969\n",
            "106 loss: 19.567102\n",
            "107 loss: 19.497421\n",
            "108 loss: 19.430593\n",
            "109 loss: 19.366558\n",
            "110 loss: 19.305248\n",
            "110 val loss: 73.27621\n",
            "111 loss: 19.246609\n",
            "112 loss: 19.190584\n",
            "113 loss: 19.137106\n",
            "114 loss: 19.08612\n",
            "115 loss: 19.037563\n",
            "116 loss: 18.991371\n",
            "117 loss: 18.947504\n",
            "118 loss: 18.905891\n",
            "119 loss: 18.866482\n",
            "120 loss: 18.829214\n",
            "120 val loss: 70.481636\n",
            "121 loss: 18.794046\n",
            "122 loss: 18.760906\n",
            "123 loss: 18.729753\n",
            "124 loss: 18.70053\n",
            "125 loss: 18.673182\n",
            "126 loss: 18.647669\n",
            "127 loss: 18.623928\n",
            "128 loss: 18.601921\n",
            "129 loss: 18.581589\n",
            "130 loss: 18.56289\n",
            "130 val loss: 68.04348\n",
            "131 loss: 18.545776\n",
            "132 loss: 18.530203\n",
            "133 loss: 18.516119\n",
            "134 loss: 18.503485\n",
            "135 loss: 18.49226\n",
            "136 loss: 18.482403\n",
            "137 loss: 18.473873\n",
            "138 loss: 18.466614\n",
            "139 loss: 18.460602\n",
            "140 loss: 18.455791\n",
            "140 val loss: 65.89558\n",
            "141 loss: 18.45215\n",
            "142 loss: 18.449635\n",
            "143 loss: 18.448206\n",
            "144 loss: 18.447845\n",
            "145 loss: 18.448494\n",
            "146 loss: 18.450138\n",
            "147 loss: 18.45274\n",
            "148 loss: 18.456263\n",
            "149 loss: 18.460676\n",
            "150 loss: 18.465948\n",
            "150 val loss: 63.982597\n",
            "151 loss: 18.472057\n",
            "152 loss: 18.478975\n",
            "153 loss: 18.486658\n",
            "154 loss: 18.495087\n",
            "155 loss: 18.50424\n",
            "156 loss: 18.514093\n",
            "157 loss: 18.524609\n",
            "158 loss: 18.53577\n",
            "159 loss: 18.547556\n",
            "160 loss: 18.559933\n",
            "160 val loss: 62.26008\n",
            "161 loss: 18.572893\n",
            "162 loss: 18.5864\n",
            "163 loss: 18.600445\n",
            "164 loss: 18.615\n",
            "165 loss: 18.630041\n",
            "166 loss: 18.645555\n",
            "167 loss: 18.66153\n",
            "168 loss: 18.67793\n",
            "169 loss: 18.694757\n",
            "170 loss: 18.711977\n",
            "170 val loss: 60.693516\n",
            "171 loss: 18.72958\n",
            "172 loss: 18.747555\n",
            "173 loss: 18.765879\n",
            "174 loss: 18.78453\n",
            "175 loss: 18.803509\n",
            "176 loss: 18.822794\n",
            "177 loss: 18.842371\n",
            "178 loss: 18.862225\n",
            "179 loss: 18.882347\n",
            "180 loss: 18.902727\n",
            "180 val loss: 59.256607\n",
            "181 loss: 18.923344\n",
            "182 loss: 18.944187\n",
            "183 loss: 18.965252\n",
            "184 loss: 18.98652\n",
            "185 loss: 19.007984\n",
            "186 loss: 19.029633\n",
            "187 loss: 19.051455\n",
            "188 loss: 19.073437\n",
            "189 loss: 19.095585\n",
            "190 loss: 19.11787\n",
            "190 val loss: 57.929443\n",
            "191 loss: 19.140299\n",
            "192 loss: 19.162853\n",
            "193 loss: 19.185526\n",
            "194 loss: 19.208307\n",
            "195 loss: 19.23119\n",
            "196 loss: 19.25417\n",
            "197 loss: 19.277235\n",
            "198 loss: 19.300383\n",
            "199 loss: 19.323599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet on MINST dataset"
      ],
      "metadata": {
        "id": "IXmTaMSipky8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  os\n",
        "import  tensorflow as tf\n",
        "import  numpy as np\n",
        "from    tensorflow import keras\n",
        "\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "tf.random.set_seed(22)\n",
        "np.random.seed(22)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "assert tf.__version__.startswith('2.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train, x_test = x_train.astype(np.float32)/255., x_test.astype(np.float32)/255.\n",
        "# [b, 28, 28] => [b, 28, 28, 1]\n",
        "x_train, x_test = np.expand_dims(x_train, axis=3), np.expand_dims(x_test, axis=3)\n",
        "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
        "# and tensors as input to keras\n",
        "y_train_ohe = tf.one_hot(y_train, depth=10).numpy()\n",
        "y_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "# 3x3 convolution\n",
        "def conv3x3(channels, stride=1, kernel=(3, 3)):\n",
        "    return keras.layers.Conv2D(channels, kernel, strides=stride, padding='same',\n",
        "                               use_bias=False,\n",
        "                            kernel_initializer=tf.random_normal_initializer())\n",
        "\n",
        "class ResnetBlock(keras.Model):\n",
        "\n",
        "    def __init__(self, channels, strides=1, residual_path=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.strides = strides\n",
        "        self.residual_path = residual_path\n",
        "\n",
        "        self.conv1 = conv3x3(channels, strides)\n",
        "        self.bn1 = keras.layers.BatchNormalization()\n",
        "        self.conv2 = conv3x3(channels)\n",
        "        self.bn2 = keras.layers.BatchNormalization()\n",
        "\n",
        "        if residual_path:\n",
        "            self.down_conv = conv3x3(channels, strides, kernel=(1, 1))\n",
        "            self.down_bn = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        residual = inputs\n",
        "\n",
        "        x = self.bn1(inputs, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # this module can be added into self.\n",
        "        # however, module in for can not be added.\n",
        "        if self.residual_path:\n",
        "            residual = self.down_bn(inputs, training=training)\n",
        "            residual = tf.nn.relu(residual)\n",
        "            residual = self.down_conv(residual)\n",
        "\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(keras.Model):\n",
        "\n",
        "    def __init__(self, block_list, num_classes, initial_filters=16, **kwargs):\n",
        "        super(ResNet, self).__init__(**kwargs)\n",
        "\n",
        "        self.num_blocks = len(block_list)\n",
        "        self.block_list = block_list\n",
        "\n",
        "        self.in_channels = initial_filters\n",
        "        self.out_channels = initial_filters\n",
        "        self.conv_initial = conv3x3(self.out_channels)\n",
        "\n",
        "        self.blocks = keras.models.Sequential(name='dynamic-blocks')\n",
        "\n",
        "        # build all the blocks\n",
        "        for block_id in range(len(block_list)):\n",
        "            for layer_id in range(block_list[block_id]):\n",
        "\n",
        "                if block_id != 0 and layer_id == 0:\n",
        "                    block = ResnetBlock(self.out_channels, strides=2, residual_path=True)\n",
        "                else:\n",
        "                    if self.in_channels != self.out_channels:\n",
        "                        residual_path = True\n",
        "                    else:\n",
        "                        residual_path = False\n",
        "                    block = ResnetBlock(self.out_channels, residual_path=residual_path)\n",
        "\n",
        "                self.in_channels = self.out_channels\n",
        "\n",
        "                self.blocks.add(block)\n",
        "\n",
        "            self.out_channels *= 2\n",
        "\n",
        "        self.final_bn = keras.layers.BatchNormalization()\n",
        "        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "\n",
        "        out = self.conv_initial(inputs)\n",
        "\n",
        "        out = self.blocks(out, training=training)\n",
        "\n",
        "        out = self.final_bn(out, training=training)\n",
        "        out = tf.nn.relu(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "def main():\n",
        "    num_classes = 10\n",
        "    batch_size = 32\n",
        "    epochs = 1\n",
        "\n",
        "    # build model and optimizer\n",
        "    model = ResNet([2, 2, 2], num_classes)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
        "                  loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    model.build(input_shape=(None, 28, 28, 1))\n",
        "    print(\"Number of variables in the model :\", len(model.variables))\n",
        "    model.summary()\n",
        "\n",
        "    # train\n",
        "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
        "\n",
        "    # evaluate on test set\n",
        "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
        "    print(\"Final test loss and accuracy :\", scores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qO8EgmbpkAl",
        "outputId": "a98fb05f-5f96-4711-d9a2-3d3b967e0261"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28, 1) (60000,)\n",
            "(10000, 28, 28, 1) (10000,)\n",
            "Number of variables in the model : 77\n",
            "Model: \"res_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           multiple                  144       \n",
            "                                                                 \n",
            " dynamic-blocks (Sequential)  (None, 7, 7, 64)         174848    \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  multiple                 256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_average_pooling2d (G  multiple                 0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense_5 (Dense)             multiple                  650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 175,898\n",
            "Trainable params: 174,874\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n",
            "1875/1875 [==============================] - 40s 19ms/step - loss: 0.4451 - accuracy: 0.8433 - val_loss: 0.4892 - val_accuracy: 0.8230\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4892 - accuracy: 0.8230\n",
            "Final test loss and accuracy : [0.4891757667064667, 0.8230000138282776]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN on IMDB dataset"
      ],
      "metadata": {
        "id": "UN92HXwPp8AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  os\n",
        "import  tensorflow as tf\n",
        "import  numpy as np\n",
        "from    tensorflow import keras\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "tf.random.set_seed(22)\n",
        "np.random.seed(22)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "assert tf.__version__.startswith('2.')\n",
        "\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 10000\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 80\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=top_words)\n",
        "# X_train = tf.convert_to_tensor(X_train)\n",
        "# y_train = tf.one_hot(y_train, depth=2)\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "\n",
        "class RNN(keras.Model):\n",
        "\n",
        "    def __init__(self, units, num_classes, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "\n",
        "        # self.cells = [keras.layers.LSTMCell(units) for _ in range(num_layers)]\n",
        "        #\n",
        "        # self.rnn = keras.layers.RNN(self.cells, unroll=True)\n",
        "        self.rnn = keras.layers.LSTM(units, return_sequences=True)\n",
        "        self.rnn2 = keras.layers.LSTM(units)\n",
        "\n",
        "        # self.cells = (keras.layers.LSTMCell(units) for _ in range(num_layers))\n",
        "        # #\n",
        "        # self.rnn = keras.layers.RNN(self.cells, return_sequences=True, return_state=True)\n",
        "        # self.rnn = keras.layers.LSTM(units, unroll=True)\n",
        "        # self.rnn = keras.layers.StackedRNNCells(self.cells)\n",
        "\n",
        "\n",
        "        # have 1000 words totally, every word will be embedding into 100 length vector\n",
        "        # the max sentence lenght is 80 words\n",
        "        self.embedding = keras.layers.Embedding(top_words, 100, input_length=max_review_length)\n",
        "        self.fc = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "\n",
        "        # print('x', inputs.shape)\n",
        "        # [b, sentence len] => [b, sentence len, word embedding]\n",
        "        x = self.embedding(inputs)\n",
        "        # print('embedding', x.shape)\n",
        "        x = self.rnn(x) \n",
        "        x = self.rnn2(x) \n",
        "        # print('rnn', x.shape)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    units = 64\n",
        "    num_classes = 2\n",
        "    batch_size = 32\n",
        "    epochs = 20\n",
        "\n",
        "    model = RNN(units, num_classes, num_layers=2)\n",
        "\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
        "                  loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # train\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(x_test, y_test), verbose=1)\n",
        "\n",
        "    # evaluate on test set\n",
        "    scores = model.evaluate(x_test, y_test, batch_size, verbose=1)\n",
        "    print(\"Final test loss and accuracy :\", scores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7mRC1Ycp7VO",
        "outputId": "a72e8fd0-441d-48d2-89df-c06ee3173e05"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Epoch 1/20\n",
            "(None, 1)\n",
            "(None, 1)\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4217 - accuracy: 0.7921(None, 1)\n",
            "782/782 [==============================] - 47s 54ms/step - loss: 0.4217 - accuracy: 0.7921 - val_loss: 0.3665 - val_accuracy: 0.8286\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.2673 - accuracy: 0.8857 - val_loss: 0.3763 - val_accuracy: 0.8424\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.1887 - accuracy: 0.9235 - val_loss: 0.4449 - val_accuracy: 0.8334\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.1296 - accuracy: 0.9508 - val_loss: 0.4835 - val_accuracy: 0.8279\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0855 - accuracy: 0.9679 - val_loss: 0.5538 - val_accuracy: 0.8234\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.0553 - accuracy: 0.9806 - val_loss: 0.7843 - val_accuracy: 0.8142\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0441 - accuracy: 0.9846 - val_loss: 0.8185 - val_accuracy: 0.8161\n",
            "Epoch 8/20\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.0321 - accuracy: 0.9887 - val_loss: 0.8608 - val_accuracy: 0.8152\n",
            "Epoch 9/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.8131 - val_accuracy: 0.8138\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.9541 - val_accuracy: 0.8185\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 0.0205 - accuracy: 0.9928 - val_loss: 0.9588 - val_accuracy: 0.8224\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 1.2382 - val_accuracy: 0.8124\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0188 - accuracy: 0.9931 - val_loss: 1.0727 - val_accuracy: 0.8205\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0145 - accuracy: 0.9949 - val_loss: 0.9984 - val_accuracy: 0.8213\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 0.0143 - accuracy: 0.9953 - val_loss: 0.9549 - val_accuracy: 0.8172\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.9297 - val_accuracy: 0.8210\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 1.0018 - val_accuracy: 0.8156\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 0.0115 - accuracy: 0.9964 - val_loss: 0.9836 - val_accuracy: 0.8213\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 1.1787 - val_accuracy: 0.8207\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 1.1747 - val_accuracy: 0.8226\n",
            "782/782 [==============================] - 12s 15ms/step - loss: 1.1747 - accuracy: 0.8226\n",
            "Final test loss and accuracy : [1.1746565103530884, 0.8226000070571899]\n"
          ]
        }
      ]
    }
  ]
}